{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "In this initial implementation, we utilized a dataset comprising over 20,000 English-Arabic sentence pairs. This dataset served as the foundation for training our Transformer-based machine translation model. The model achieved an average BLEU score of approximately 0.23, indicating that it could capture basic translation patterns but had limitations in handling more complex or nuanced sentences.\n",
        "\n",
        "A significant challenge was the dataset's limited size and diversity. While 20,000 sentences provide a starting point, it is relatively small compared to the extensive corpora used in state-of-the-art machine translation systems, which often involve millions of sentence pairs. This limitation restricts the model's exposure to a wide range of vocabulary, idiomatic expressions, and varied syntactic structures. Such exposure is crucial, especially when translating between English and Arabic, due to their linguistic differences and the morphological richness of Arabic.\n",
        "\n",
        "To enhance the model's performance, it is essential to acquire more extensive and diverse datasets. Additional high-quality data would enable the model to learn a broader vocabulary and better understand context, leading to more accurate and fluent translations. Moreover, increasing computational resources would allow for training deeper models with larger embedding dimensions and more Transformer layers, which could capture the complexities of both languages more effectively.\n",
        "\n",
        "Investing in these areas—expanding the dataset and utilizing more computational power—would address the current limitations. It would significantly improve the model's ability to handle complex sentence structures, idiomatic expressions, and the nuanced linguistic patterns necessary for high-quality machine translation between English and Arabic."
      ],
      "metadata": {
        "id": "iMU5RFKKNaSD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)\n"
      ],
      "metadata": {
        "id": "WwouRog5NcCO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aaeda6c9-383a-43a5-b065-20895333bfe3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Data Preparation\n",
        "Tokenization:\n",
        "\n",
        "Tokenize the English and Arabic sentences separately.\n",
        "Consider using subword tokenization (e.g., Byte-Pair Encoding or WordPiece) to handle out-of-vocabulary words. This is especially useful for Arabic, as it has complex morphology.\n",
        "Vocabulary Creation:\n",
        "\n",
        "Create separate vocabularies for English and Arabic tokens.\n",
        "Map each token to a unique integer for use in the model.\n",
        "Padding and Batching:\n",
        "\n",
        "Pad sequences to a fixed length to handle variable-length inputs and outputs.\n",
        "Group similar-length sentences in batches to optimize training speed and memory use."
      ],
      "metadata": {
        "id": "l93azZWqNfN9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reading data from ara_eng.txt\n",
        "english_sentences = []\n",
        "arabic_sentences = []\n",
        "\n",
        "with open('ara_eng.txt', 'r', encoding='utf-8') as file:\n",
        "    for line in file:\n",
        "        # Assuming sentences are separated by a tab (adjust if needed)\n",
        "        eng, ara = line.strip().split('\\t')\n",
        "        english_sentences.append(eng)\n",
        "        arabic_sentences.append(ara)\n",
        "\n",
        "print(\"Sample English Sentence:\", english_sentences[0])\n",
        "print(\"Sample Arabic Sentence:\", arabic_sentences[0])\n"
      ],
      "metadata": {
        "id": "_KCx5KLuNgKD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "058ca3c8-d863-4b5b-bdb9-ae45da4f60e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample English Sentence: Hi.\n",
            "Sample Arabic Sentence: مرحبًا.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save English and Arabic sentences to separate temporary files\n",
        "with open('english_sentences.txt', 'w', encoding='utf-8') as eng_file:\n",
        "    for sentence in english_sentences:\n",
        "        eng_file.write(sentence + '\\n')\n",
        "\n",
        "with open('arabic_sentences.txt', 'w', encoding='utf-8') as ara_file:\n",
        "    for sentence in arabic_sentences:\n",
        "        ara_file.write(sentence + '\\n')\n"
      ],
      "metadata": {
        "id": "TGqzY_YrO8jg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sentencepiece as spm\n",
        "\n",
        "# Train English tokenizer\n",
        "spm.SentencePieceTrainer.Train(input='english_sentences.txt', model_prefix='eng_tokenizer', vocab_size=8000)\n",
        "\n",
        "# Train Arabic tokenizer\n",
        "spm.SentencePieceTrainer.Train(input='arabic_sentences.txt', model_prefix='ara_tokenizer', vocab_size=8000)\n"
      ],
      "metadata": {
        "id": "FVIxjzXdPALz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the trained tokenizers\n",
        "sp_eng = spm.SentencePieceProcessor(model_file='eng_tokenizer.model')\n",
        "sp_ara = spm.SentencePieceProcessor(model_file='ara_tokenizer.model')\n",
        "\n",
        "# Tokenize the first few sentences to verify\n",
        "eng_tokens = [sp_eng.encode(sentence, out_type=int) for sentence in english_sentences]\n",
        "ara_tokens = [sp_ara.encode(sentence, out_type=int) for sentence in arabic_sentences]\n",
        "\n",
        "print(\"Tokenized English:\", eng_tokens[:2])\n",
        "print(\"Tokenized Arabic:\", ara_tokens[:2])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R8BABltHPGbs",
        "outputId": "5d9b343c-fc7d-4188-f23b-39a312a45dbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized English: [[5434, 4], [13, 0, 890, 288]]\n",
            "Tokenized Arabic: [[6541, 362, 8], [37, 3650, 293]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get vocabulary sizes\n",
        "eng_vocab_size = sp_eng.get_piece_size()\n",
        "ara_vocab_size = sp_ara.get_piece_size()\n",
        "\n",
        "# Create dictionaries for each language\n",
        "eng_id_to_token = {i: sp_eng.id_to_piece(i) for i in range(eng_vocab_size)}\n",
        "ara_id_to_token = {i: sp_ara.id_to_piece(i) for i in range(ara_vocab_size)}\n",
        "\n",
        "# Reverse mappings\n",
        "eng_token_to_id = {v: k for k, v in eng_id_to_token.items()}\n",
        "ara_token_to_id = {v: k for k, v in ara_id_to_token.items()}\n"
      ],
      "metadata": {
        "id": "d7JxFSShPNM_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# Convert lists of tokens to PyTorch tensors\n",
        "eng_tensors = [torch.tensor(tokens) for tokens in eng_tokens]\n",
        "ara_tensors = [torch.tensor(tokens) for tokens in ara_tokens]\n",
        "\n",
        "# Pad sequences\n",
        "eng_padded = pad_sequence(eng_tensors, batch_first=True, padding_value=0)\n",
        "ara_padded = pad_sequence(ara_tensors, batch_first=True, padding_value=0)\n",
        "\n",
        "print(\"Padded English Sentences:\\n\", eng_padded)\n",
        "print(\"Padded Arabic Sentences:\\n\", ara_padded)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ATg7eS5cPRPt",
        "outputId": "ae73fc9e-ece6-4db0-bad4-49018de2a59b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Padded English Sentences:\n",
            " tensor([[5434,    4,    0,  ...,    0,    0,    0],\n",
            "        [  13,    0,  890,  ...,    0,    0,    0],\n",
            "        [ 102,  153,  185,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [ 236,  172,  125,  ...,    0,    0,    0],\n",
            "        [   6,  151,  152,  ...,    0,    0,    0],\n",
            "        [  21,   54,   78,  ...,    0,    0,    0]])\n",
            "Padded Arabic Sentences:\n",
            " tensor([[6541,  362,    8,  ...,    0,    0,    0],\n",
            "        [  37, 3650,  293,  ...,    0,    0,    0],\n",
            "        [7852,  293,    0,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [ 600,   30,   99,  ...,    0,    0,    0],\n",
            "        [2715,    6, 7124,  ...,    0,    0,    0],\n",
            "        [ 144,  729,  108,  ...,    0,    0,    0]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Create a dataset and dataloader\n",
        "dataset = TensorDataset(eng_padded, ara_padded)\n",
        "data_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Sample batch\n",
        "for eng_batch, ara_batch in data_loader:\n",
        "    print(\"English Batch:\\n\", eng_batch)\n",
        "    print(\"Arabic Batch:\\n\", ara_batch)\n",
        "    break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EjLo-YYPPWMp",
        "outputId": "1e3fa5da-cabc-4c90-8f00-b9dbaa17a703"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English Batch:\n",
            " tensor([[  63,   54,  131,  ...,    0,    0,    0],\n",
            "        [ 641,    3,  354,  ...,    0,    0,    0],\n",
            "        [ 739, 1218,   46,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [ 102, 4131,   11,  ...,    0,    0,    0],\n",
            "        [2501, 1279, 6443,  ...,    0,    0,    0],\n",
            "        [  35,  152,    7,  ...,    0,    0,    0]])\n",
            "Arabic Batch:\n",
            " tensor([[7054,   55,   67,  ...,    0,    0,    0],\n",
            "        [ 510,    3, 4227,  ...,    0,    0,    0],\n",
            "        [  59,   53,   30,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [   3, 4741, 5973,  ...,    0,    0,    0],\n",
            "        [ 471, 1260, 4488,  ...,    0,    0,    0],\n",
            "        [  88,  560,    7,  ...,    0,    0,    0]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece\n",
        "!pip install nltk\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jcrGcMx8Zez8",
        "outputId": "dd7e5880-2ef5-4323-db18-5337b93acbfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.2.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MuKngTUxZfNU",
        "outputId": "7e6bf724-28b1-43bd-86b5-15160cfd50f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vNi_X-eXZnL9",
        "outputId": "e47a1c98-31a7-4462-e135-9b44e41111de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Full code from beginning to end\n",
        "\n",
        "# 1. Imports and Setup\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import math\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import sentencepiece as spm\n",
        "import random\n",
        "import numpy as np\n",
        "import gc\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "# Check if CUDA is available and set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')\n",
        "\n",
        "# 2. Data Preparation\n",
        "\n",
        "# Reading data from ara_eng.txt\n",
        "english_sentences = []\n",
        "arabic_sentences = []\n",
        "\n",
        "with open('ara_eng.txt', 'r', encoding='utf-8') as file:\n",
        "    for line in file:\n",
        "        # Assuming sentences are separated by a tab\n",
        "        eng, ara = line.strip().split('\\t')\n",
        "        english_sentences.append(eng)\n",
        "        arabic_sentences.append(ara)\n",
        "\n",
        "print(\"Sample English Sentence:\", english_sentences[0])\n",
        "print(\"Sample Arabic Sentence:\", arabic_sentences[0])\n",
        "\n",
        "# Optional: Limit dataset size for testing (uncomment if needed)\n",
        "# english_sentences = english_sentences[:10000]\n",
        "# arabic_sentences = arabic_sentences[:10000]\n",
        "\n",
        "# 3. Tokenization with SentencePiece\n",
        "\n",
        "# Save English and Arabic sentences to separate files\n",
        "with open('english_sentences.txt', 'w', encoding='utf-8') as eng_file:\n",
        "    for sentence in english_sentences:\n",
        "        eng_file.write(sentence + '\\n')\n",
        "\n",
        "with open('arabic_sentences.txt', 'w', encoding='utf-8') as ara_file:\n",
        "    for sentence in arabic_sentences:\n",
        "        ara_file.write(sentence + '\\n')\n",
        "\n",
        "# Train SentencePiece tokenizers\n",
        "spm.SentencePieceTrainer.Train(input='english_sentences.txt', model_prefix='eng_tokenizer', vocab_size=8000)\n",
        "spm.SentencePieceTrainer.Train(input='arabic_sentences.txt', model_prefix='ara_tokenizer', vocab_size=8000)\n",
        "\n",
        "# Load the trained tokenizers\n",
        "sp_eng = spm.SentencePieceProcessor(model_file='eng_tokenizer.model')\n",
        "sp_ara = spm.SentencePieceProcessor(model_file='ara_tokenizer.model')\n",
        "\n",
        "# Clean up to save memory\n",
        "del english_sentences, arabic_sentences\n",
        "gc.collect()\n",
        "\n",
        "# 4. Dataset and DataLoader\n",
        "\n",
        "# Define maximum sequence length\n",
        "max_seq_length = 100  # Adjust as needed\n",
        "\n",
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, src_file, tgt_file, tokenizer_src, tokenizer_tgt, max_length):\n",
        "        self.src_sentences = open(src_file, 'r', encoding='utf-8').readlines()\n",
        "        self.tgt_sentences = open(tgt_file, 'r', encoding='utf-8').readlines()\n",
        "        self.tokenizer_src = tokenizer_src\n",
        "        self.tokenizer_tgt = tokenizer_tgt\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.src_sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src_sentence = self.src_sentences[idx].strip()\n",
        "        tgt_sentence = self.tgt_sentences[idx].strip()\n",
        "        src_tokens = self.tokenizer_src.encode(src_sentence, out_type=int)[:self.max_length]\n",
        "        tgt_tokens = self.tokenizer_tgt.encode(tgt_sentence, out_type=int)[:self.max_length]\n",
        "        return torch.tensor(src_tokens), torch.tensor(tgt_tokens)\n",
        "\n",
        "def collate_fn(batch):\n",
        "    src_batch, tgt_batch = zip(*batch)\n",
        "    src_batch = pad_sequence(src_batch, batch_first=True, padding_value=0)\n",
        "    tgt_batch = pad_sequence(tgt_batch, batch_first=True, padding_value=0)\n",
        "    return src_batch, tgt_batch\n",
        "\n",
        "# Create dataset and data loaders\n",
        "dataset = TranslationDataset('english_sentences.txt', 'arabic_sentences.txt', sp_eng, sp_ara, max_seq_length)\n",
        "\n",
        "# Split dataset into training and validation sets\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "\n",
        "# Create data loaders with reduced batch size\n",
        "batch_size = 16  # Adjust as needed\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=collate_fn)\n",
        "\n",
        "# 5. Model Definition\n",
        "\n",
        "# Embedding and Positional Encoding Layers\n",
        "class EmbeddingLayer(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size):\n",
        "        super(EmbeddingLayer, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.embedding(x)\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, embed_size, dropout=0.1, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        pe = torch.zeros((max_len, embed_size))\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, embed_size, 2).float() * (-math.log(10000.0) / embed_size))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)  # Shape: [1, max_len, embed_size]\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, :x.size(1)]\n",
        "        return self.dropout(x)\n",
        "\n",
        "# Transformer Model Definition\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size, embed_size=256, num_heads=4, num_layers=2, dropout=0.1):\n",
        "        super(TransformerModel, self).__init__()\n",
        "\n",
        "        self.src_embedding = EmbeddingLayer(src_vocab_size, embed_size)\n",
        "        self.tgt_embedding = EmbeddingLayer(tgt_vocab_size, embed_size)\n",
        "        self.positional_encoding = PositionalEncoding(embed_size, dropout)\n",
        "\n",
        "        self.transformer = nn.Transformer(\n",
        "            d_model=embed_size,\n",
        "            nhead=num_heads,\n",
        "            num_encoder_layers=num_layers,\n",
        "            num_decoder_layers=num_layers,\n",
        "            dropout=dropout,\n",
        "            batch_first=True  # Set batch_first to True\n",
        "        )\n",
        "\n",
        "        self.fc_out = nn.Linear(embed_size, tgt_vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def create_padding_mask(self, sequences, pad_idx=0):\n",
        "        return (sequences == pad_idx)\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        src_padding_mask = self.create_padding_mask(src)\n",
        "        tgt_padding_mask = self.create_padding_mask(tgt)\n",
        "        memory_key_padding_mask = src_padding_mask.clone()\n",
        "\n",
        "        src = self.positional_encoding(self.src_embedding(src))\n",
        "        tgt = self.positional_encoding(self.tgt_embedding(tgt))\n",
        "\n",
        "        output = self.transformer(\n",
        "            src,\n",
        "            tgt,\n",
        "            src_key_padding_mask=src_padding_mask,\n",
        "            tgt_key_padding_mask=tgt_padding_mask,\n",
        "            memory_key_padding_mask=memory_key_padding_mask\n",
        "        )\n",
        "        output = self.fc_out(output)\n",
        "        return output\n",
        "\n",
        "# 6. Training and Evaluation Functions\n",
        "\n",
        "# Initialize the model\n",
        "src_vocab_size = sp_eng.get_piece_size()\n",
        "tgt_vocab_size = sp_ara.get_piece_size()\n",
        "model = TransformerModel(src_vocab_size, tgt_vocab_size, embed_size=256, num_heads=4, num_layers=2, dropout=0.1)\n",
        "model.to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)  # Ignore padding index during loss calculation\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.95)\n",
        "\n",
        "# Training function\n",
        "def train(model, data_loader, criterion, optimizer, scheduler, num_epochs=10, pad_idx=0):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_loss = 0\n",
        "        for src_batch, tgt_batch in data_loader:\n",
        "            src_batch = src_batch.to(device)\n",
        "            tgt_batch = tgt_batch.to(device)\n",
        "\n",
        "            tgt_input = tgt_batch[:, :-1]\n",
        "            tgt_output = tgt_batch[:, 1:]\n",
        "\n",
        "            # Forward pass\n",
        "            output = model(src_batch, tgt_input)\n",
        "\n",
        "            # Reshape output and target for loss calculation\n",
        "            output = output.reshape(-1, output.shape[-1])\n",
        "            tgt_output = tgt_output.reshape(-1)\n",
        "\n",
        "            loss = criterion(output, tgt_output)\n",
        "\n",
        "            # Backward pass\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            # Accumulate loss\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        scheduler.step()\n",
        "        avg_epoch_loss = epoch_loss / len(data_loader)\n",
        "        print(f'Epoch {epoch + 1}, Loss: {avg_epoch_loss:.4f}')\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate(model, data_loader, pad_idx=0):\n",
        "    model.eval()\n",
        "    total_bleu_score = 0\n",
        "    total_sentences = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for src_batch, tgt_batch in data_loader:\n",
        "            src_batch = src_batch.to(device)\n",
        "            tgt_batch = tgt_batch.to(device)\n",
        "\n",
        "            tgt_input = tgt_batch[:, :-1]\n",
        "            tgt_output = tgt_batch[:, 1:]\n",
        "\n",
        "            # Generate prediction\n",
        "            output = model(src_batch, tgt_input)\n",
        "            output = output.argmax(dim=-1)\n",
        "\n",
        "            # Calculate BLEU for each sentence\n",
        "            for i in range(tgt_output.size(0)):\n",
        "                reference = [tgt_output[i].tolist()]\n",
        "                candidate = output[i].tolist()\n",
        "                # Remove padding tokens\n",
        "                reference = [[token for token in ref if token != pad_idx] for ref in reference]\n",
        "                candidate = [token for token in candidate if token != pad_idx]\n",
        "                total_bleu_score += sentence_bleu(reference, candidate)\n",
        "                total_sentences += 1\n",
        "\n",
        "    avg_bleu_score = total_bleu_score / total_sentences\n",
        "    print(f'Average BLEU Score: {avg_bleu_score:.4f}')\n",
        "\n",
        "# 7. Training Execution\n",
        "\n",
        "# Train the model\n",
        "train(model, train_loader, criterion, optimizer, scheduler, num_epochs=10, pad_idx=0)\n",
        "\n",
        "# Evaluate the model\n",
        "evaluate(model, val_loader, pad_idx=0)\n",
        "\n",
        "# Optional: Save the model\n",
        "torch.save(model.state_dict(), 'transformer_model.pth')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EhjtJFpLZrWc",
        "outputId": "903a8af3-0934-4118-ef68-cd4ee55ce422"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Sample English Sentence: Hi.\n",
            "Sample Arabic Sentence: مرحبًا.\n",
            "Epoch 1, Loss: 5.8839\n",
            "Epoch 2, Loss: 3.3133\n",
            "Epoch 3, Loss: 1.7921\n",
            "Epoch 4, Loss: 1.1230\n",
            "Epoch 5, Loss: 0.7994\n",
            "Epoch 6, Loss: 0.5994\n",
            "Epoch 7, Loss: 0.4718\n",
            "Epoch 8, Loss: 0.3761\n",
            "Epoch 9, Loss: 0.3071\n",
            "Epoch 10, Loss: 0.2537\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. (Triggered internally at ../aten/src/ATen/NestedTensorImpl.cpp:178.)\n",
            "  output = torch._nested_tensor_from_mask(\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average BLEU Score: 0.2313\n"
          ]
        }
      ]
    }
  ]
}
